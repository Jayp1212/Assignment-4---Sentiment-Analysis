{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13943e6b",
   "metadata": {},
   "source": [
    "# AIDI  1006 : AI Infrastucture and Architecture\n",
    "# Assignment 4: Sentiment Analysis\n",
    "# Jay Babulal Patel : 200543276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcaf9b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles from BBC world...\n",
      "Fetching articles from BBC Asia...\n",
      "Fetching articles from BBC UK...\n",
      "Fetching articles from BBC Business...\n",
      "Fetching articles from BBC Politics...\n",
      "Fetching articles from BBC Health...\n",
      "Fetching articles from BBC Science...\n",
      "Fetching articles from BBC Technology...\n",
      "Articles saved to scraped_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import feedparser\n",
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "\n",
    "# Load RSS feeds from JSON file\n",
    "with open('NewsPapers.json', 'r') as file:\n",
    "    newspapers = json.load(file)['newspapers']\n",
    "\n",
    "# Function to fetch and parse article content\n",
    "def fetch_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            tree = html.fromstring(response.content)\n",
    "            content = tree.xpath('//*[@id=\"main-content\"]/article/div[3]')\n",
    "            if content:\n",
    "                return content[0].text_content().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Initialize list to hold article data\n",
    "articles = []\n",
    "\n",
    "# Iterate through each newspaper and fetch articles\n",
    "for name, info in newspapers.items():\n",
    "    print(f\"Fetching articles from {name}...\")\n",
    "    feed = feedparser.parse(info['rss'])\n",
    "    for entry in feed.entries:\n",
    "        title = entry.title\n",
    "        link = entry.link\n",
    "        published = entry.published if 'published' in entry else 'N/A'\n",
    "        content = fetch_article_content(link)\n",
    "        if content:\n",
    "            articles.append({\n",
    "                'newspaper': name,\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'published': published,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(articles)\n",
    "df.to_csv('scraped_articles.csv', index=False)\n",
    "print(\"Articles saved to scraped_articles.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a02bf30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and saved to scraped_articles_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('scraped_articles.csv')\n",
    "\n",
    "# Function to get sentiment polarity and subjectivity\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Apply sentiment analysis to the content\n",
    "df['polarity'], df['subjectivity'] = zip(*df['content'].apply(analyze_sentiment))\n",
    "\n",
    "# Save the DataFrame with sentiment analysis results to a new CSV file\n",
    "df.to_csv('scraped_articles_with_sentiment.csv', index=False)\n",
    "print(\"Sentiment analysis completed and saved to scraped_articles_with_sentiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0115ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('scraped_articles_with_sentiment.csv')\n",
    "\n",
    "# Save the CSV file with proper quoting\n",
    "df.to_csv('scraped_articles_with_sentiment_corrected.csv', index=False, quoting=csv.QUOTE_ALL, quotechar='\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a56935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('scraped_articles_with_sentiment.csv')\n",
    "\n",
    "# Convert 'published' column to datetime format\n",
    "df['published'] = pd.to_datetime(df['published'], format='%a, %d %b %Y %H:%M:%S %Z')\n",
    "\n",
    "# Extract date in the desired format\n",
    "df['published'] = df['published'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b8fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scraped_articles_with_sentiment_corrected_modified.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffba2f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been successfully cleaned and saved to data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def remove_quotes_and_commas(input_file, output_file, delimiter=','):\n",
    "    try:\n",
    "        cleaned_rows = []\n",
    "        problematic_rows = []\n",
    "\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "            reader = csv.reader(infile, delimiter=delimiter, quotechar='\"')\n",
    "            headers = next(reader)\n",
    "            cleaned_rows.append(headers)\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                cleaned_row = [cell.replace('\"', '').replace(',', '') for cell in row]\n",
    "                if len(cleaned_row) != len(headers):\n",
    "                    problematic_rows.append((i + 2, row))\n",
    "                cleaned_rows.append(cleaned_row)\n",
    "\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter=delimiter, quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerows(cleaned_rows)\n",
    "        \n",
    "        print(f\"CSV file has been successfully cleaned and saved to {output_file}\")\n",
    "        if problematic_rows:\n",
    "            print(f\"Problematic rows found and cleaned: {problematic_rows}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing the CSV file: {e}\")\n",
    "\n",
    "input_file = 'scraped_articles_with_sentiment_corrected_modified.csv'\n",
    "output_file = 'data.csv'\n",
    "remove_quotes_and_commas(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
